<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.10.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Openssl Guides: Datagram BIO API revisions for sendmmsg/recvmmsg</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Openssl Guides
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.10.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Datagram BIO API revisions for sendmmsg/recvmmsg</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>We need to evolve the API surface of BIO which is relevant to BIO_dgram (and the eventual BIO_dgram_mem) to support APIs which allow multiple datagrams to be sent or received simultaneously, such as sendmmsg(2)/recvmmsg(2).</p>
<h1><a class="anchor" id="autotoc_md258"></a>
The adopted design</h1>
<h2><a class="anchor" id="autotoc_md259"></a>
Design decisions</h2>
<p>The adopted design makes the following design decisions:</p>
<ul>
<li>We use a sendmmsg/recvmmsg-like API. The alternative API was not considered for adoption because it is an explicit goal that the adopted API be suitable for concurrent use on the same BIO.</li>
<li>We define our own structures rather than using the OS's <code>struct mmsghdr</code>. The motivations for this are:<ul>
<li>It ensures portability between OSes and allows the API to be used on OSes which do not support <code>sendmmsg</code> or <code>sendmsg</code>.</li>
<li>It allows us to use structures in keeping with OpenSSL's existing abstraction layers (e.g. <code>BIO_ADDR</code> rather than <code>struct sockaddr</code>).</li>
<li>We do not have to expose functionality which we cannot guarantee we can support on all platforms (for example, arbitrary control messages).</li>
<li>It avoids the need to include OS headers in our own public headers, which would pollute the environment of applications which include our headers, potentially undesirably.</li>
</ul>
</li>
<li>For OSes which do not support <code>sendmmsg</code>, we emulate it using repeated calls to <code>sendmsg</code>. For OSes which do not support <code>sendmsg</code>, we emulate it using <code>sendto</code> to the extent feasible. This avoids the need for code consuming these new APIs to define a fallback code path.</li>
<li>We do not define any flags at this time, as the flags previously considered for adoption cannot be supported on all platforms (Win32 does not have <code>MSG_DONTWAIT</code>).</li>
<li>We ensure the extensibility of our <code>BIO_MSG</code> structure in a way that preserves ABI compatibility using a <code>stride</code> argument which callers must set to <code>sizeof(BIO_MSG)</code>. Implementations can examine the stride field to determine whether a given field is part of a <code>BIO_MSG</code>. This allows us to add optional fields to <code>BIO_MSG</code> at a later time without breaking ABI. All new fields must be added to the end of the structure.</li>
<li><p class="startli">The BIO methods are designed to support stateless operation in which they are simply calls to the equivalent system calls, where supported, without changing BIO state. In particular, this means that things like retry flags are not set or cleared by <code>BIO_sendmmsg</code> or <code>BIO_recvmmsg</code>.</p>
<p class="startli">The motivation for this is that these functions are intended to support concurrent use on the same BIO. If they read or modify BIO state, they would need to be synchronised with a lock, undermining performance on what (for <code>BIO_dgram</code>) would otherwise be a straight system call.</p>
</li>
<li>We do not support iovecs. The motivations for this are:<ul>
<li>Not all platforms can support iovecs (e.g. Windows).</li>
<li>The only way we could emulate iovecs on platforms which don't support them is by copying the data to be sent into a staging buffer. This would defeat all of the advantages of iovecs and prevent us from meeting our zero/single-copy requirements. Moreover, it would lead to extremely surprising performance variations for consumers of the API.</li>
<li>We do not believe iovecs are needed to meet our performance requirements for QUIC. The reason for this is that aside from a minimal packet header, all data in QUIC is encrypted, so all data sent via QUIC must pass through an encrypt step anyway, meaning that all data sent will already be copied and there is not going to be any issue depositing the ciphertext in a staging buffer together with the frame header.</li>
<li>Even if we did support iovecs, we would have to impose a limit on the number of iovecs supported, because we translate from our own structures (as discussed above) and also intend these functions to be stateless and not requiire locking. Therefore the OS-native iovec structures would need to be allocated on the stack.</li>
</ul>
</li>
<li><p class="startli">Sometimes, an application may wish to learn the local interface address associated with a receive operation or specify the local interface address to be used for a send operation. We support this, but require this functionality to be explicitly enabled before use.</p>
<p class="startli">The reason for this is that enabling this functionality generally requires that the socket be reconfigured using <code>setsockopt</code> on most platforms. Doing this on-demand would require state in the BIO to determine whether this functionality is currently switched on, which would require otherwise unnecessary locking, undermining performance in concurrent usage of this API on a given BIO. By requiring this functionality to be enabled explicitly before use, this allows this initialization to be done up front without performance cost. It also aids users of the API to understand that this functionality is not always available and to detect when this functionality is available in advance.</p>
</li>
</ul>
<h2><a class="anchor" id="autotoc_md260"></a>
Design</h2>
<p>The currently proposed design is as follows:</p>
<div class="fragment"><div class="line"><span class="keyword">typedef</span> <span class="keyword">struct </span>bio_msg_st {</div>
<div class="line">    <span class="keywordtype">void</span> *data;</div>
<div class="line">    <span class="keywordtype">size_t</span> data_len;</div>
<div class="line">    BIO_ADDR *peer, *local;</div>
<div class="line">    uint64_t flags;</div>
<div class="line">} BIO_MSG;</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#define BIO_UNPACK_ERRNO(e)     </span><span class="comment">/*...*/</span><span class="preprocessor"></span></div>
<div class="line"><span class="preprocessor">#define BIO_IS_ERRNO(e)         </span><span class="comment">/*...*/</span><span class="preprocessor"></span></div>
<div class="line"> </div>
<div class="line">ossl_ssize_t BIO_sendmmsg(BIO *<a class="code hl_struct" href="structb.html">b</a>, BIO_MSG *msg, <span class="keywordtype">size_t</span> stride,</div>
<div class="line">                          <span class="keywordtype">size_t</span> num_msg, uint64_t flags);</div>
<div class="line">ossl_ssize_t BIO_recvmmsg(BIO *<a class="code hl_struct" href="structb.html">b</a>, BIO_MSG *msg, <span class="keywordtype">size_t</span> stride,</div>
<div class="line">                          <span class="keywordtype">size_t</span> num_msg, uint64_t flags);</div>
<div class="ttc" id="astructb_html"><div class="ttname"><a href="structb.html">b</a></div><div class="ttdef"><b>Definition</b> check-format-test-negatives.c:375</div></div>
</div><!-- fragment --><p>The API is used as follows:</p>
<ul>
<li><code>msg</code> points to an array of <code>num_msg</code> <code>BIO_MSG</code> structures.</li>
<li>Both functions have identical prototypes, and return the number of messages processed in the array. If no messages were sent due to an error, <code>-1</code> is returned. If an OS-level socket error occurs, a negative value <code>v</code> is returned. The caller should determine that <code>v</code> is an OS-level socket error by calling <code>BIO_IS_ERRNO(v)</code> and may obtain the OS-level socket error code by calling <code>BIO_UNPACK_ERRNO(v)</code>.</li>
<li><code>stride</code> must be set to <code>sizeof(BIO_MSG)</code>.</li>
<li><code>data</code> points to the buffer of data to be sent or to be filled with received data. <code>data_len</code> is the size of the buffer in bytes on call. If the given message in the array is processed (i.e., if the return value exceeds the index of that message in the array), <code>data_len</code> is updated to the actual amount of data sent or received at return time.</li>
<li><code>flags</code> in the <code>BIO_MSG</code> structure provides per-message flags to the <code>BIO_sendmmsg</code> or <code>BIO_recvmmsg</code> call. If the given message in the array is processed, <code>flags</code> is written with zero or more result flags at return time. The <code>flags</code> argument to the call itself provides for global flags affecting all messages in the array. Currently, no per-message or global flags are defined and all of these fields are set to zero on call and on return.</li>
<li><p class="startli"><code>peer</code> and <code>local</code> are optional pointers to <code>BIO_ADDR</code> structures into which the remote and local addresses are to be filled. If either of these are NULL, the given addressing information is not requested. Local address support may not be available in all circumstances, in which case processing of the message fails. (This means that the function returns the number of messages processed, or -1 if the message in question is the first message.)</p>
<p class="startli">Support for <code>local</code> must be explicitly enabled before use, otherwise attempts to use it fail.</p>
</li>
</ul>
<p>Local address support is enabled as follows:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> BIO_dgram_set_local_addr_enable(BIO *<a class="code hl_struct" href="structb.html">b</a>, <span class="keywordtype">int</span> enable);</div>
<div class="line"><span class="keywordtype">int</span> BIO_dgram_get_local_addr_enable(BIO *<a class="code hl_struct" href="structb.html">b</a>);</div>
<div class="line"><span class="keywordtype">int</span> BIO_dgram_get_local_addr_cap(BIO *<a class="code hl_struct" href="structb.html">b</a>);</div>
</div><!-- fragment --><p><code>BIO_dgram_get_local_addr_cap()</code> returns 1 if local address support is available. It is then enabled using <code>BIO_dgram_set_local_addr_enable()</code>, which fails if support is not available.</p>
<h1><a class="anchor" id="autotoc_md261"></a>
Options which were considered</h1>
<p>Options for the API surface which were considered included:</p>
<h2><a class="anchor" id="autotoc_md262"></a>
sendmmsg/recvmmsg-like API</h2>
<p>This design was chosen to form the basis of the adopted design, which is described above.</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> BIO_readm(BIO *<a class="code hl_struct" href="structb.html">b</a>, BIO_mmsghdr *msgvec,</div>
<div class="line">              <span class="keywordtype">unsigned</span> len, <span class="keywordtype">int</span> flags, <span class="keyword">struct</span> timespec *timeout);</div>
<div class="line"><span class="keywordtype">int</span> BIO_writem(BIO *<a class="code hl_struct" href="structb.html">b</a>, BIO_mmsghdr *msgvec,</div>
<div class="line">              <span class="keywordtype">unsigned</span> len, <span class="keywordtype">int</span> flags, <span class="keyword">struct</span> timespec *timeout);</div>
</div><!-- fragment --><p>We can either define <code>BIO_mmsghdr</code> as a typedef of <code>struct mmsghdr</code> or redefine an equivalent structure. The former has the advantage that we can just pass the structures through to the syscall without copying them.</p>
<p>Note that in <code>BIO_mem_dgram</code> we will have to process and therefore understand the contents of <code>struct mmsghdr</code> ourselves. Therefore, initially we define a subset of <code>struct mmsghdr</code> as being supported, specifically no control messages; <code>msg_name</code> and <code>msg_iov</code> only.</p>
<p>The flags argument is defined by us. Initially we can support something like <code>MSG_DONTWAIT</code> (say, <code>BIO_DONTWAIT</code>).</p>
<h3><a class="anchor" id="autotoc_md263"></a>
Implementation Questions</h3>
<p>If we go with this, there are some issues that arise:</p>
<ul>
<li>Are <code>BIO_mmsghdr</code>, <code>BIO_msghdr</code> and <code>BIO_iovec</code> simple typedefs for OS-provided structures, or our own independent structure definitions?<ul>
<li>If we use OS-provided structures:<ul>
<li>We would need to include the OS headers which provide these structures in our public API headers.</li>
<li>If we choose to support these functions when OS support is not available (see discussion below), We would need to define our own structures in this case (a “polyfill” approach).</li>
</ul>
</li>
<li>If we use our own structures:<ul>
<li><p class="startli">We would need to translate these structures during every call.</p>
<p class="startli">But we would need to have storage inside the BIO_dgram for <em>m</em> <code>struct msghdr</code>, <em>m*v</em> iovecs, etc. Since we want to support multithreaded use these allocations probably will need to be on the stack, and therefore must be limited.</p>
<p class="startli">Limiting <em>m</em> isn't a problem, because <code>sendmmsg</code> returns the number of messages sent, so the existing semantics we are trying to match lets us just send or receive fewer messages than we were asked to.</p>
<p class="startli">However, it does seem like we will need to limit <em>v</em>, the number of iovecs per message. So what limit should we give to <em>v</em>, the number of iovecs? We will need a fixed stack allocation of OS iovec structures and we can allocate from this stack allocation as we iterate through the <code>BIO_msghdr</code> we have been given. So in practice we could just only send messages until we reach our iovec limit, and then return.</p>
<p class="startli">For example, suppose we allocate 64 iovecs internally:</p>
<p class="startli"><code>c struct iovec vecs[64]; </code></p>
<p class="startli">If the first message passed to a call to <code>BIO_writem</code> has 64 iovecs attached to it, no further messages can be sent and <code>BIO_writem</code> returns 1.</p>
<p class="startli">If three messages are sent, with 32, 32, and 1 iovecs respectively, the first two messages are sent and <code>BIO_writem</code> returns 2.</p>
<p class="startli">So the only important thing we would need to document in this API is the limit of iovecs on a single message; in other words, the number of iovecs which must not be exceeded if a forward progress guarantee is to be made. e.g. if we allocate 64 iovecs internally, <code>BIO_writem</code> with a single message with 65 iovecs will never work and this becomes part of the API contract.</p>
<p class="startli">Obviously these quantities of iovecs are unrealistically large. iovecs are small, so we can afford to set the limit high enough that it shouldn't cause any problems in practice. We can increase the limit later without a breaking API change, but we cannot decrease it later. So we might want to start with something small, like 8.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>We also need to decide what to do for OSes which don't support at least <code>sendmsg</code>/<code>recvmsg</code>.<ul>
<li>Don't provide these functions and require all users of these functions to have an alternate code path which doesn't rely on them?<ul>
<li>Not providing these functions on OSes that don't support at least sendmsg/recvmsg is a simple solution but adds complexity to code using BIO_dgram. (Though it does communicate to code more realistic performance expectations since it knows when these functions are actually available.)</li>
</ul>
</li>
<li>Provide these functions and emulate the functionality:<ul>
<li>However there is a question here as to how we implement the iovec arguments on platforms without <code>sendmsg</code>/<code>recvmsg</code>. (We cannot use <code>writev</code>/<code>readv</code> because we need peer address information.) Logically implementing these would then have to be done by copying buffers around internally before calling <code>sendto</code>/<code>recvfrom</code>, defeating the point of iovecs and providing a performance profile which is surprising to code using BIO_dgram.</li>
<li>Another option could be a variable limit on the number of iovecs, which can be queried from BIO_dgram. This would be a constant set when libcrypto is compiled. It would be 1 for platforms not supporting <code>sendmsg</code>/<code>recvmsg</code>. This again adds burdens on the code using BIO_dgram, but it seems the only way to avoid the surprising performance pitfall of buffer copying to emulate iovec support. There is a fair risk of code being written which accidentally works on one platform but not another, because the author didn't realise the iovec limit is 1 on some platforms. Possibly we could have an “iovec limit” variable in the BIO_dgram which is 1 by default, which can be increased by a call to a function BIO_set_iovec_limit, but not beyond the fixed size discussed above. It would return failure if not possible and this would give client code a clear way to determine if its expectations are met.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="autotoc_md264"></a>
Alternate API</h2>
<p>Could we use a simplified API? For example, could we have an API that returns one datagram where BIO_dgram uses <code>readmmsg</code> internally and queues the returned datagrams, thereby still avoiding extra syscalls but offering a simple API.</p>
<p>The problem here is we want to support “single-copy” (where the data is only copied as it is decrypted). Thus BIO_dgram needs to know the final resting place of encrypted data at the time it makes the <code>readmmsg</code> call.</p>
<p>One option would be to allow the user to set a callback on BIO_dgram it can use to request a new buffer, then have an API which returns the buffer:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> BIO_dgram_set_read_callback(BIO *<a class="code hl_struct" href="structb.html">b</a>,</div>
<div class="line">                                <span class="keywordtype">void</span> *(*cb)(<span class="keywordtype">size_t</span> len, <span class="keywordtype">void</span> *arg),</div>
<div class="line">                                <span class="keywordtype">void</span> *arg);</div>
<div class="line"><span class="keywordtype">int</span> BIO_dgram_set_read_free_callback(BIO *<a class="code hl_struct" href="structb.html">b</a>,</div>
<div class="line">                                     <span class="keywordtype">void</span> (*cb)(<span class="keywordtype">void</span> *buf,</div>
<div class="line">                                                <span class="keywordtype">size_t</span> buf_len,</div>
<div class="line">                                                <span class="keywordtype">void</span> *arg),</div>
<div class="line">                                     <span class="keywordtype">void</span> *arg);</div>
<div class="line"><span class="keywordtype">int</span> BIO_read_dequeue(BIO *<a class="code hl_struct" href="structb.html">b</a>, <span class="keywordtype">void</span> **buf, <span class="keywordtype">size_t</span> *buf_len);</div>
</div><!-- fragment --><p>The BIO_dgram calls the specified callback when it needs to generate internal iovecs for its <code>readmmsg</code> call, and the received datagrams can then be popped by the application and freed as it likes. (The read free callback above is only used in rare circumstances, such as when calls to <code>BIO_read</code> and <code>BIO_read_dequeue</code> are alternated, or when the BIO_dgram is destroyed prior to all read buffers being dequeued; see below.) For convenience we could have an extra call to allow a buffer to be pushed back into the BIO_dgram's internal queue of unused read buffers, which avoids the need for the application to do its own management of such recycled buffers:</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> BIO_dgram_push_read_buffer(BIO *<a class="code hl_struct" href="structb.html">b</a>, <span class="keywordtype">void</span> *buf, <span class="keywordtype">size_t</span> buf_len);</div>
</div><!-- fragment --><p>On the write side, the application provides buffers and can get a callback when they are freed. BIO_write_queue just queues for transmission, and the <code>sendmmsg</code> call is made when calling <code>BIO_flush</code>. (TBD: whether it is reasonable to overload the semantics of BIO_flush in this way.)</p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> BIO_dgram_set_write_done_callback(BIO *<a class="code hl_struct" href="structb.html">b</a>,</div>
<div class="line">                                      <span class="keywordtype">void</span> (*cb)(<span class="keyword">const</span> <span class="keywordtype">void</span> *buf,</div>
<div class="line">                                                 <span class="keywordtype">size_t</span> buf_len,</div>
<div class="line">                                                 <span class="keywordtype">int</span> status,</div>
<div class="line">                                                 <span class="keywordtype">void</span> *arg),</div>
<div class="line">                                      <span class="keywordtype">void</span> *arg);</div>
<div class="line"><span class="keywordtype">int</span> BIO_write_queue(BIO *<a class="code hl_struct" href="structb.html">b</a>, <span class="keyword">const</span> <span class="keywordtype">void</span> *buf, <span class="keywordtype">size_t</span> buf_len);</div>
<div class="line"><span class="keywordtype">int</span> BIO_flush(BIO *<a class="code hl_struct" href="structb.html">b</a>);</div>
</div><!-- fragment --><p>The status argument to the write done callback will be 1 on success, some negative value on failure, and some special negative value if the BIO_dgram is being freed before the write could be completed.</p>
<p>For send/receive addresses, we import the <code>BIO_(set|get)_dgram_(origin|dest)</code> APIs proposed in the sendmsg/recvmsg PR (#5257). <code>BIO_get_dgram_(origin|dest)</code> should be called immediately after <code>BIO_read_dequeue</code> and <code>BIO_set_dgram_(origin|dest)</code> should be called immediately before <code>BIO_write_queue</code>.</p>
<p>This approach allows <code>BIO_dgram</code> to support myriad options via composition of successive function calls in a “builder” style rather than via a single function call with an excessive number of arguments or pointers to unwieldy ever-growing argument structures, requiring constant revision of the central read/write functions of the BIO API.</p>
<p>Note that since <code>BIO_set_dgram_(origin|dest)</code> sets data on outgoing packets and <code>BIO_get_dgram_(origin|dest)</code> gets data on incoming packets, it doesn't follow that these are accessing the same data (they are not setters and getters of a variables called "dgram origin" and "dgram destination", even though they look like setters and getters of the same variables from the name.) We probably want to separate these as there is no need for a getter for outgoing packet destination, for example, and by separating these we allow the possibility of multithreaded use (one thread reads, one thread writes) in the future. Possibly we should choose less confusing names for these functions. Maybe <code>BIO_set_outgoing_dgram_(origin|dest)</code> and <code>BIO_get_incoming_dgram_(origin|dest)</code>.</p>
<p>Pros of this approach:</p>
<ul>
<li><p class="startli">Application can generate one datagram at a time and still get the advantages of sendmmsg/recvmmsg (fewer syscalls, etc.)</p>
<p class="startli">We probably want this for our own QUIC implementation built on top of this anyway. Otherwise we will need another piece to do basically the same thing and agglomerate multiple datagrams into a single BIO call. Unless we only want use <code>sendmmsg</code> constructively in trivial cases (e.g. where we send two datagrams from the same function immediately after one another... doesn't seem like a common use case.)</p>
</li>
<li>Flexible support for single-copy (zero-copy).</li>
</ul>
<p>Cons of this approach:</p>
<ul>
<li>Very different way of doing reads/writes might be strange to existing applications. <em>But</em> the primary consumer of this new API will be our own QUIC implementation so probably not a big deal. We can always support <code>BIO_read</code>/<code>BIO_write</code> as a less efficient fallback for existing third party users of BIO_dgram.</li>
</ul>
<h3><a class="anchor" id="autotoc_md265"></a>
Compatibility interop</h3>
<p>Suppose the following sequence happens:</p>
<ol type="1">
<li>BIO_read (legacy call path)</li>
<li>BIO_read_dequeue (<code>recvmmsg</code> based call path with callback-allocated buffer)</li>
<li>BIO_read (legacy call path)</li>
</ol>
<p>For (1) we have two options</p>
<p>a. Use <code>recvmmsg</code> and add the received datagrams to an RX queue just as for the <code>BIO_read_dequeue</code> path. We use an OpenSSL-provided default allocator (<code>OPENSSL_malloc</code>) and flag these datagrams as needing to be freed by OpenSSL, not the application.</p>
<p>When the application calls <code>BIO_read</code>, a copy is performed and the internal buffer is freed.</p>
<p>b. Use <code>recvfrom</code> directly. This means we have a <code>recvmmsg</code> path and a <code>recvfrom</code> path depending on what API is being used.</p>
<p>The disadvantage of (a) is it yields an extra copy relative to what we have now, whereas with (b) the buffer passed to <code>BIO_read</code> gets passed through to the syscall and we do not have to copy anything.</p>
<p>Since we will probably need to support platforms without <code>sendmmsg</code>/<code>recvmmsg</code> support anyway, (b) seems like the better option.</p>
<p>For (2) the new API is used. Since the previous call to BIO_read is essentially “stateless” (it's just a simple call to <code>recvfrom</code>, and doesn't require mutation of any internal BIO state other than maybe the last datagram source/destination address fields), BIO_dgram can go ahead and start using the <code>recvmmsg</code> code path. Since the RX queue will obviously be empty at this point, it is initialised and filled using <code>recvmmsg</code>, then one datagram is popped from it.</p>
<p>For (3) we have a legacy <code>BIO_read</code> but we have several datagrams still in the RX queue. In this case we do have to copy - we have no choice. However this only happens in circumstances where a user of BIO_dgram alternates between old and new APIs, which should be very unusual.</p>
<p>Subsequently for (3) we have to free the buffer using the free callback. This is an unusual case where BIO_dgram is responsible for freeing read buffers and not the application (the only other case being premature destruction, see below). But since this seems a very strange API usage pattern, we may just want to fail in this case.</p>
<p>Probably not worth supporting this. So we can have the following rule:</p>
<ul>
<li>After the first call to <code>BIO_read_dequeue</code> is made on a BIO_dgram, all subsequent calls to ordinary <code>BIO_read</code> will fail.</li>
</ul>
<p>Of course, all of the above applies analogously to the TX side.</p>
<h3><a class="anchor" id="autotoc_md266"></a>
BIO_dgram_pair</h3>
<p>We will also implement from scratch a BIO_dgram_pair. This will be provided as a BIO pair which provides identical semantics to the BIO_dgram above, both for the legacy and zero-copy code paths.</p>
<h3><a class="anchor" id="autotoc_md267"></a>
Thread safety</h3>
<p>It is a functional assumption of the above design that we would never want to have more than one thread doing TX on the same BIO and never have more than one thread doing RX on the same BIO.</p>
<p>If we did ever want to do this, multiple BIOs on the same FD is one possibility (for the BIO_dgram case at least). But I don't believe there is any general intention to support multithreaded use of a single BIO at this time (unless I am mistaken), so this seems like it isn't an issue.</p>
<p>If we wanted to support multithreaded use of the same FD using the same BIO, we would need to revisit the set-call-then-execute-call API approach above (<code>BIO_(set|get)_dgram_(origin|dest)</code>) as this would pose a problem. But I mainly mention this only for completeness. Our recent learnt lessons on cache contention suggest that this probably wouldn't be a good idea anyway.</p>
<h3><a class="anchor" id="autotoc_md268"></a>
Other questions</h3>
<p>BIO_dgram will call the allocation function to get buffers for <code>recvmmsg</code> to fill. We might want to have a way to specify how many buffers it should offer to <code>recvmmsg</code>, and thus how many buffers it allocates in advance.</p>
<h3><a class="anchor" id="autotoc_md269"></a>
Premature destruction</h3>
<p>If BIO_dgram is freed before all datagrams are read, the read buffer free callback is used to free any unreturned read buffers. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.10.0
</small></address>
</body>
</html>
